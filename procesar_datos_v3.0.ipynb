{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RFSExZh5BbTcb9hh3CihDliHJwnS6hcv",
      "authorship_tag": "ABX9TyNeeGFShDkJ1RRTaMgCt1av",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jorge774/Procesamiento_datos/blob/main/procesar_datos_v3.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''procesar_datos.py\n",
        "   Este programa hace lo siguiente:\n",
        "   1.Reavisa la carpeta donde estan los archivos txt que genera janus luego de hacer adquisicion.\n",
        "   2.Revisa y toma los archivos txt que no ha leido anteriormente.\n",
        "   3.Toma el encabezado del archivo RunXX_list.txt con menor XX de todo el lote de archivos tomados en 2.\n",
        "   4.Toma como referencia temporal el 'Run start time' del encabezado tomado en 3.\n",
        "   5.Genera una estampa temporal para cada evento de todos los archivos RunXX_list.txt tomando como referencia a 4.\n",
        "   6.Genera un archivo 'MuTe_20240304_11h07m01s.txt'\n",
        "   7.Elimina aquellos eventos donde el conteo en todos los canales fue 0.\n",
        "   8.Elimina los archivos RunXX_list.txt\n",
        "\n",
        "   Los parametros de este programa son leidos en el archivo 'config.txt', los cuales son:\n",
        "   1.Ruta absoluta donde quedan los archivos txt que genera janus luego de hacer adquisicion.\n",
        "   2.Ruta absoluta donde quedaran los archivos 'MuTe_20240304_11h07m01s.txt'.\n",
        "   3.Numero de canales usados en la adquision.\n",
        "   4.Numero de lineas usadas en los encabezados de los archivos 'RunXX_list.txt'.\n",
        "\n",
        "   En el archivo 'vars.txt' el programa escribe el nombre de los archivos 'RunXX_list.txt' que va procesando.\n",
        "'''"
      ],
      "metadata": {
        "id": "tzXh47hUjytD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBsPiHLt0DOU"
      },
      "outputs": [],
      "source": [
        "print('Procesando...')\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "from pytz import timezone\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "ini_exe=datetime.now(timezone('America/Bogota'))\n",
        "config_file=open(file='config.txt',mode='r');output_input_folders=config_file.read();config_file.close()\n",
        "num_header_lines=int(output_input_folders.split('\\n')[3].split('#')[0])#8\n",
        "#n_channels=int(output_input_folders.split('\\n')[2].split('#')[0])#64 #numero de canales\n",
        "folder_path_caen=output_input_folders.split('\\n')[0].split('#')[0]\n",
        "output_folder_path=output_input_folders.split('\\n')[1].split('#')[0]\n",
        "pos_mute=output_input_folders.split('\\n')[4].split('#')[0]\n",
        "padding=sys.argv[1].split()[0]#hacer padding en el txt, string s o n\n",
        "csv=sys.argv[1].split()[1]#generar csv string s o n\n",
        "batch=int(sys.argv[1].split()[2])#tamaño del lote de archivos de corrida"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gestor de errores en los archivos run list\n",
        "\n",
        "#eliminar archivos.dat\n",
        "\n",
        "#ordenar los files_to_proces de acuerdo al run start time\n",
        "def key_function(file,folder_path_caen):\n",
        "    if re.search('list.txt',file):\n",
        "       file_path=folder_path_caen+'/'+file;file_to_proces=open(file=file_path,mode='r')\n",
        "       content=file_to_proces.readlines()[:10];file_to_proces.close()\n",
        "       string_date_time=''\n",
        "       for i in content[6].split()[4:-1]:i+=' ';string_date_time+=i\n",
        "       #datetime.strptime('Fri Mar 1 16:32:05 2024 ','%a %b %d %H:%M:%S %Y ')\n",
        "       ref_date_time=datetime.strptime(string_date_time,'%a %b %d %H:%M:%S %Y ')-timedelta(hours=5)\n",
        "       ref_date_time=pd.Timestamp(str(ref_date_time))\n",
        "       del content\n",
        "    if re.search('Info.txt',file):\n",
        "       file_path=folder_path_caen+'/'+file;file_to_proces=open(file=file_path,mode='r')\n",
        "       content=file_to_proces.readlines()[:10];file_to_proces.close()\n",
        "       ref_date_time=content[3].split()[2]+' '+content[3].split()[3]\n",
        "       ref_date_time=pd.Timestamp(datetime.strptime(ref_date_time,'%d/%m/%Y %H:%M'))\n",
        "       del content\n",
        "    #ignorar folders y archivos que no sean de info o corrida\n",
        "    if not re.search('list.txt',file) and not re.search('Info.txt',file):ref_date_time=pd.Timestamp('2030-01-01 00:00:00')\n",
        "    return ref_date_time\n",
        "\n",
        "#verificar que un archivo de corrida tenga informacion de todos los canales en todas las ventanas de adquision\n",
        "def check_arvhivo_corrida(file,folder_path_caen,num_header_lines,n_channels):\n",
        "    print('Verificando archivo de corrida...')\n",
        "    check=True#archivo de corrida sin errores\n",
        "    file_path=folder_path_caen+'/'+file;file_to_proces=open(file=file_path,mode='r')\n",
        "    data_and_header=file_to_proces.readlines();file_to_proces.close()\n",
        "    ini_index=num_header_lines+1\n",
        "    end_index=num_header_lines+n_channels+1\n",
        "    while ini_index<(len(data_and_header)-1):\n",
        "          #recorrer todos los data blocks de un archivo Runxx_list.txt\n",
        "          data_block=data_and_header[ini_index:end_index]\n",
        "          chs_in_acq=[int(i.split()[1]) for i in data_block]\n",
        "          if len(chs_in_acq)!=n_channels:\n",
        "             check=False#archivo de corrida con errores\n",
        "             #os.remove(file_path)#eliminar archivo runxxlits.txt corrupto\n",
        "             print('El archivo '+file+' esta corrupto.')\n",
        "             break\n",
        "          ini_index=end_index\n",
        "          end_index=ini_index+n_channels\n",
        "    del data_and_header\n",
        "    return check\n",
        "\n",
        "#infiere el numero de chs en adquision. Supone que se adquirió almenos un evento\n",
        "def inferir_n_channels(file,folder_path_caen,num_header_lines):\n",
        "    #infiere el numero de chs en adquision. Supone que se adquirió almenos un evento\n",
        "    file_path=folder_path_caen+'/'+file;file_to_proces=open(file=file_path,mode='r')\n",
        "    data_and_header=file_to_proces.readlines();file_to_proces.close()\n",
        "    n_channels=1;ch_index=num_header_lines+2\n",
        "    while len(data_and_header[ch_index].split())<=3:\n",
        "          n_channels+=1;ch_index+=1\n",
        "          try:\n",
        "            data_and_header[ch_index]\n",
        "          except IndexError:\n",
        "                break\n",
        "    print('El archivo '+file+' tiene informacion de '+str(n_channels)+' canales.')\n",
        "    return n_channels\n",
        "\n",
        "#cambiar el nombre de los archivos Runx_List.txt e Runx_Info.txt\n",
        "def rename_file_folder_path_caen(folder_path_caen,file,current_time_stamp):\n",
        "    #1.verificar si es un archivo info o un archivo run_list\n",
        "    #2.generar string con la fecha y hora\n",
        "    #3.cambiar el nobre\n",
        "    file_path=folder_path_caen+'/'+file\n",
        "    if re.search('list.txt',file):\n",
        "       new_name=folder_path_caen+'/'+'MUTE'+'_'+pos_mute+'_'+current_time_stamp+'.raw'\n",
        "       os.rename(file_path,new_name)\n",
        "    if re.search('Info.txt',file):\n",
        "       new_name=folder_path_caen+'/'+'MUTE'+'_'+pos_mute+'_'+current_time_stamp+'.raw.mtd'\n",
        "       os.rename(file_path,new_name)\n",
        "\n",
        "#se asume que para cada archivo de corrida hay un archivo de info\n",
        "def armar_lote_archivos_de_corrida(batch):\n",
        "    temp=[];batch_info=batch;batch_run=batch\n",
        "    for i in files_to_proces:\n",
        "        if re.search('list.txt',i) and batch_run>0:\n",
        "           batch_run-=1;temp.append(i)\n",
        "        if re.search('Info.txt',i) and batch_info>0:\n",
        "           batch_info-=1;temp.append(i)\n",
        "    return temp"
      ],
      "metadata": {
        "id": "y1TZ4wA-XioM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path_vars='vars.txt'\n",
        "##ini main loop\n",
        "vars_file=open(file=folder_path_vars,mode='r')\n",
        "processed_file_list=vars_file.read()\n",
        "vars_file.close()\n",
        "current_file_list=os.listdir(folder_path_caen)\n",
        "\n",
        "if processed_file_list:\n",
        "   #processed_file_list is not empty\n",
        "   files_to_proces=list(set(current_file_list)-set(processed_file_list.split('\\n')))#archivos que esten en folder_path_caen pero que no esten en el vars file\n",
        "else:\n",
        "   #processed_file_list is empty\n",
        "   files_to_proces=current_file_list\n",
        "\n",
        "files_to_proces.sort(key=lambda file:key_function(file,folder_path_caen))#ordenar los files_to_proces de acuerdo al run start time\n",
        "\n",
        "#aqui se agarra x cantidad de archivos run_list y x cantidad de archivos Info para procesarlos\n",
        "files_to_proces=armar_lote_archivos_de_corrida(batch)\n",
        "\n",
        "if files_to_proces:\n",
        "    #files_to_proces is not empty\n",
        "    count_files_run_list_processed=0\n",
        "    #formato v1:'MuTe_20240304_11h07m01s'\n",
        "    #formato v2:MUTE_MACH1_DATE-TIME.dat\n",
        "    output_file_path=''\n",
        "    gmt_5_datetime=datetime.now(timezone('America/Bogota'))\n",
        "    cur_h='0'+str(gmt_5_datetime.hour) if gmt_5_datetime.hour<10 else str(gmt_5_datetime.hour)\n",
        "    cur_day='0'+str(gmt_5_datetime.day) if gmt_5_datetime.day<10 else str(gmt_5_datetime.day)\n",
        "    cur_mon='0'+str(gmt_5_datetime.month) if gmt_5_datetime.month<10 else str(gmt_5_datetime.month)\n",
        "    cur_min='0'+str(gmt_5_datetime.minute) if gmt_5_datetime.minute<10 else str(gmt_5_datetime.minute)\n",
        "    cur_sec='0'+str(gmt_5_datetime.second) if gmt_5_datetime.second<10 else str(gmt_5_datetime.second)\n",
        "    current_time_stamp=str(gmt_5_datetime.year)+cur_mon+cur_day+'_'+cur_h+'h'+cur_min+'m'+cur_sec+'s'\n",
        "    name='MUTE'+'_'+pos_mute+'_'+current_time_stamp\n",
        "    #name='MUTE_MACH1_'+str(gmt_5_datetime.year)+cur_mon+cur_day+'_'+cur_h+'h'+cur_min+'m'+cur_sec+'s'\n",
        "    files_to_compress=[]\n",
        "    print('Escribiendo archivo de salida...')\n",
        "    for file in files_to_proces:\n",
        "        print('Leyendo archivo '+file+'...')\n",
        "        if re.search('list.txt',file):n_channels=inferir_n_channels(file,folder_path_caen,num_header_lines)\n",
        "        if re.search('list.txt',file) and check_arvhivo_corrida(file,folder_path_caen,num_header_lines,n_channels):\n",
        "           #procesar los datos adquiridos\n",
        "           #se procesa un archivo Runxx_list.txt de files_to_proces\n",
        "           #ejecutar con cada archivo Runxx_list.txt\n",
        "           file_path=folder_path_caen+'/'+file\n",
        "           file_to_proces=open(file=file_path,mode='r')\n",
        "           data_and_header=file_to_proces.readlines()\n",
        "           file_to_proces.close()\n",
        "           #current_time_stamp debe ser la que trae el archivo de corrida\n",
        "           file_time_stamp=key_function(file,folder_path_caen)\n",
        "           file_time_stamp_str=str(file_time_stamp.year)+str(file_time_stamp.month)+str(file_time_stamp.day)+'_'+str(file_time_stamp.hour)+'h'+str(file_time_stamp.minute)+'m'+str(file_time_stamp.second)+'s'\n",
        "           #cambiar nombre de archivos de corrida\n",
        "           rename_file_folder_path_caen(folder_path_caen,file,file_time_stamp_str)\n",
        "           output_file_path=output_folder_path+'/'+name+'.dat'\n",
        "           output_file=open(file=output_file_path,mode='a')\n",
        "\n",
        "           header=data_and_header[:num_header_lines]\n",
        "           if count_files_run_list_processed==0:\n",
        "              string_date_time=''\n",
        "              for i in header[6].split()[4:-1]:i+=' ';string_date_time+=i\n",
        "              #datetime.strptime('Fri Mar 1 16:32:05 2024 ','%a %b %d %H:%M:%S %Y ')\n",
        "              ref_date_time=datetime.strptime(string_date_time,'%a %b %d %H:%M:%S %Y ')-timedelta(hours=5)\n",
        "              ref_date_time=pd.Timestamp(str(ref_date_time))\n",
        "              delta_time=ref_date_time-ref_date_time\n",
        "              temp=data_and_header[num_header_lines+1:num_header_lines+n_channels+1]\n",
        "              chs_in_acq=[int(i.split()[1]) for i in temp]\n",
        "              aux=['ch0'+str(i) if i<10 else 'ch'+str(i) for i in chs_in_acq]\n",
        "              aux2=[aux[i]+' ' if i!=(len(aux)-1) else aux[i] for i in range(len(aux))]\n",
        "              header2='Tstamp GMT-5(aa-mm-dd hh:mm:ss.ns) '\n",
        "              for i in aux2:header2+=i\n",
        "              output_file.write(header2+'\\n')\n",
        "           else:\n",
        "              string_date_time=''\n",
        "              for i in header[6].split()[4:-1]:i+=' ';string_date_time+=i\n",
        "              #datetime.strptime('Fri Mar 1 16:32:05 2024 ','%a %b %d %H:%M:%S %Y ')\n",
        "              last_date_time=datetime.strptime(string_date_time,'%a %b %d %H:%M:%S %Y ')-timedelta(hours=5)\n",
        "              last_date_time=pd.Timestamp(str(last_date_time))\n",
        "              delta_time=last_date_time-ref_date_time\n",
        "\n",
        "           ini_index=num_header_lines+1\n",
        "           end_index=num_header_lines+n_channels+1\n",
        "           while ini_index<(len(data_and_header)-1):\n",
        "                #recorrer todos los data blocks de un archivo Runxx_list.txt\n",
        "                data_block=data_and_header[ini_index:end_index]\n",
        "                #ejeutar para cada data block\n",
        "                ch_counts=[]\n",
        "                for i in range(len(data_block)):ch_counts.append(data_block[i].split()[2])\n",
        "                if ch_counts.count('0')!=n_channels:#verificar que almenos un ch se haya activado\n",
        "                    Tstamp_us=data_block[0].split()[3]\n",
        "                    nanosegundos=(delta_time.total_seconds()*1e+9)+(float(Tstamp_us)*1000)\n",
        "                    Tstamp_gmt_5=ref_date_time+pd.Timedelta(nanosegundos,unit='ns')\n",
        "                    output_file.write(str(Tstamp_gmt_5))\n",
        "                    for _ in range(36-len(str(Tstamp_gmt_5))):output_file.write(' ')\n",
        "                    for i in range(len(data_block)):\n",
        "                        count=data_block[i].split()[2]\n",
        "                        if i!=(len(data_block)-1):\n",
        "                           output_file.write(count)\n",
        "                           for _ in range(4+1-len(list(count))):output_file.write(' ')\n",
        "                        else:\n",
        "                           output_file.write(count+'\\n')\n",
        "                ini_index=end_index\n",
        "                end_index=ini_index+n_channels\n",
        "           output_file.close()\n",
        "           count_files_run_list_processed+=1\n",
        "           files_to_compress.append(folder_path_caen+'/'+'MUTE'+'_'+pos_mute+'_'+file_time_stamp_str+'.raw')#archivo de corrida marcado para comprimir\n",
        "           #os.remove(file_path)#eliminar archivo runxxlits.txt\n",
        "           del data_and_header#liberar memoria\n",
        "        if re.search('Info.txt',file):\n",
        "           #procesar metadata de los datos adquiridos\n",
        "           #current_time_stamp debe ser la que trae el archivo de Info\n",
        "           file_time_stamp=key_function(file,folder_path_caen)\n",
        "           file_time_stamp_str=str(file_time_stamp.year)+str(file_time_stamp.month)+str(file_time_stamp.day)+'_'+str(file_time_stamp.hour)+'h'+str(file_time_stamp.minute)+'m'+str(file_time_stamp.second)+'s'\n",
        "           #cambiarn nombre de archivo de Info\n",
        "           rename_file_folder_path_caen(folder_path_caen,file,file_time_stamp_str)\n",
        "           files_to_compress.append(folder_path_caen+'/'+'MUTE'+'_'+pos_mute+'_'+file_time_stamp_str+'.raw.mtd')#archivo de Info marcado para comprimir\n",
        "    if os.path.exists(output_file_path):\n",
        "        output_file=open(file=output_file_path,mode='r');content_output_file=output_file.readlines();output_file.close()\n",
        "        if not content_output_file[1:]:\n",
        "           os.remove(output_file_path)#eliminar txt si esta vacio\n",
        "        else:\n",
        "            if csv=='s' or padding=='s':\n",
        "                #aqui si arma un csv con los datos\n",
        "                all_64_channels=64\n",
        "                aux=['ch0'+str(i) if i<10 else 'ch'+str(i) for i in range(all_64_channels)]\n",
        "                chs_in_acq=content_output_file[0].split()[3:]\n",
        "                chs_in_acq_num=[int(i[2:]) for i in chs_in_acq]\n",
        "                event_values=content_output_file[1:]\n",
        "                data=[];to_txt=[]\n",
        "                for k in event_values:\n",
        "                    chs_in_acq_values=[int(l) for l in k.split()[2:]]\n",
        "                    all_chs_values=[];j=0\n",
        "                    for i in range(all_64_channels):\n",
        "                        if not i in chs_in_acq_num:\n",
        "                          #ejecutar si i no esta en chs_in_acq_num\n",
        "                          all_chs_values.append(0)\n",
        "                        else:\n",
        "                          #ejecutar si i esta en chs_in_acq_num\n",
        "                          all_chs_values.append(chs_in_acq_values[j])\n",
        "                          j+=1\n",
        "                    all_chs_values_str=''\n",
        "                    for t in range(len(all_chs_values)):\n",
        "                        if  t!=(all_64_channels-1):\n",
        "                            all_chs_values_str=all_chs_values_str+str(all_chs_values[t])+'    '\n",
        "                        else:\n",
        "                            all_chs_values_str=all_chs_values_str+str(all_chs_values[t])\n",
        "                    row=k[:36]+all_chs_values_str+'\\n'\n",
        "                    to_txt.append(row)\n",
        "                    if csv=='s':\n",
        "                      #filas del dataframe\n",
        "                      row_data_frame=[pd.Timestamp(row.split()[0]+' '+row.split()[1])]+[int(g) for g in row.split()[2:]]\n",
        "                      data.append(row_data_frame)\n",
        "                      #filas del dataframe\n",
        "                if csv=='s':\n",
        "                    print('Armando csv...')\n",
        "                    name_columns=['time']+aux\n",
        "                    df=pd.DataFrame(data,columns=name_columns)#armar dataframe\n",
        "                    csv_output_name=output_folder_path+'/'+name+'.csv'\n",
        "                    df.to_csv(csv_output_name,index=False)\n",
        "                    files_to_compress.append(csv_output_name)#archivo csv de salida marcado para comprimir\n",
        "                    del data\n",
        "                if padding=='s':\n",
        "                    #se sobre escribe el txt de salida con paddings\n",
        "                    print('Rellenando con 0 los chs que no se adquirieron.')\n",
        "                    aux2=[aux[i]+' ' if i!=(len(aux)-1) else aux[i] for i in range(len(aux))]\n",
        "                    header2='Tstamp GMT-5(aa-mm-dd hh:mm:ss.ns) '\n",
        "                    for i in aux2:header2+=i\n",
        "                    header2=header2+'\\n'\n",
        "                    to_txt.insert(0,header2)\n",
        "                    output_file=open(file=output_file_path,mode='w');output_file.writelines(to_txt);output_file.close()\n",
        "                    del to_txt\n",
        "            files_to_compress.append(output_file_path)#archivo de salida marcado para comprimir\n",
        "        del content_output_file\n",
        "    vars_file=open(file=folder_path_vars,mode='a')\n",
        "    if processed_file_list:\n",
        "       #processed_file_list is not empty\n",
        "       vars_file.write('\\n')\n",
        "       for i in range(0,len(files_to_proces)):\n",
        "           if i!=(len(files_to_proces)-1):\n",
        "              vars_file.write(files_to_proces[i]+'\\n')\n",
        "           else:\n",
        "              vars_file.write(files_to_proces[i])\n",
        "    else:\n",
        "      #processed_file_list is empty\n",
        "      for i in range(0,len(files_to_proces)):\n",
        "          if i!=(len(files_to_proces)-1):\n",
        "              vars_file.write(files_to_proces[i]+'\\n')\n",
        "          else:\n",
        "              vars_file.write(files_to_proces[i])\n",
        "    vars_file.close()\n",
        "    #Generar metadata del archivo .dat que creó este codigo.\n",
        "    #En este punto del codigo se genera un archivo de metada por lote de archivos de corrida procesado\n",
        "    meta_data_file_path=output_folder_path+'/'+name+'.dat.mtd'\n",
        "\n",
        "    n_raws_files=0;n_raws_mtd_files=0\n",
        "    for j in range(len(files_to_compress)):\n",
        "        if re.findall('.raw$',files_to_compress[j]):n_raws_files+=1\n",
        "        if re.findall('.raw.mtd$',files_to_compress[j]):n_raws_mtd_files+=1\n",
        "    content_tar_file='Este archivo tar contiene '+str(len(files_to_compress)+1)+' archivos:\\n'\\\n",
        "                    +'-El presente archivo de metadata de el/los archivo/os crudo/os procesado/os(dat.mtd)\\n'\\\n",
        "                    +'-1 archivo de datos crudos procesados(.dat)\\n'\\\n",
        "                    +'-'+str(n_raws_files)+' archivo/os de dato/os crudo/os(.raw)\\n'\\\n",
        "                    +'-'+str(n_raws_mtd_files)+' archivo/os de metada de archivo/os crudo/os(.raw.mtd)\\n'\n",
        "    meta_data=[content_tar_file,'Codigo usado para procesar datos crudos: https://gitmilab.redclara.net/muografia/MuTeInstrument/mute-2024-analisis-codigos.git\\n'\\\n",
        "              ,'Desarrollado por: Jorge L. Perea\\n','Correo: jorge.perea@correo.uis.edu.co\\n']\n",
        "    if csv=='s':meta_data.insert(1,'-1 archivo csv que contiene la misma informacion del .dat\\n')\n",
        "    meta_data_file=open(file=meta_data_file_path,mode='w');meta_data_file.writelines(meta_data);meta_data_file.close()\n",
        "\n",
        "    files_to_compress.append(meta_data_file_path)#archivo de metada del arvhivo de salida marcado para compromir\n",
        "    #Comprimir archivos\n",
        "    #files_to_compress contiene:\n",
        "    #-archivo .raw\n",
        "    #-archivo .raw.mtd\n",
        "    #-archivo .dat\n",
        "    #-archivo .dat.mtd\n",
        "    print('Comprimiendo archivos...')\n",
        "    tar = tarfile.open(output_folder_path+'/'+name+'.tar.gz','w:gz')\n",
        "    for file in files_to_compress:\n",
        "        tar.add(file,arcname=os.path.basename(file))\n",
        "        os.remove(file)\n",
        "    tar.close()\n",
        "print('Procesamiento terminado. Duracion del procesamiento:',(datetime.now(timezone('America/Bogota'))-ini_exe).total_seconds(),'segundos')\n",
        "##end main loop"
      ],
      "metadata": {
        "id": "Jqf5HT8fHxfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generar pixel map"
      ],
      "metadata": {
        "id": "BD53Df-MZ5mT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}